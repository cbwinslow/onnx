[
    {
        "model": "BERT-Squad",
        "model_path": "text/machine_comprehension/bert-squad/model/bertsquad-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "cad65b9807a5e0393e4f84331f9a0c5c844d9cc736e39781a80f9c48ca39447c",
            "model_bytes": 435882893,
            "tags": [
                "text",
                "machine comprehension",
                "bert-squad"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "unique_ids_raw_output___9:0",
                        "shape": [
                            "unk__475"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "segment_ids:0",
                        "shape": [
                            "unk__476",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_mask:0",
                        "shape": [
                            "unk__477",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_ids:0",
                        "shape": [
                            "unk__478",
                            256
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "unstack:1",
                        "shape": [
                            "unk__479",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unstack:0",
                        "shape": [
                            "unk__480",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unique_ids:0",
                        "shape": [
                            "unk__481"
                        ],
                        "type": "tensor(int64)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/bert-squad/model/bertsquad-8.tar.gz",
            "model_with_data_sha": "c8c6c7e0ab9e1333b86e8415a9d990b2570f9374f80be1c1cb72f182d266f666",
            "model_with_data_bytes": 403400046
        }
    },
    {
        "model": "BERT-Squad",
        "model_path": "text/machine_comprehension/bert-squad/model/bertsquad-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "5945dee6478abdab2d5e4ce3868b4d969741e3dad2134cc669da65a4f092755b",
            "model_bytes": 435852734,
            "tags": [
                "text",
                "machine comprehension",
                "bert-squad"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "unique_ids_raw_output___9:0",
                        "shape": [
                            "unk__492"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "segment_ids:0",
                        "shape": [
                            "unk__493",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_mask:0",
                        "shape": [
                            "unk__494",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_ids:0",
                        "shape": [
                            "unk__495",
                            256
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "unstack:1",
                        "shape": [
                            "unk__496",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unstack:0",
                        "shape": [
                            "unk__497",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unique_ids:0",
                        "shape": [
                            "unk__498"
                        ],
                        "type": "tensor(int64)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/bert-squad/model/bertsquad-10.tar.gz",
            "model_with_data_sha": "5ed61ab22c90db6928e88fad21f72217b04d6b5ed249c92de46321f77a6f35c3",
            "model_with_data_bytes": 403081843
        }
    },
    {
        "model": "GPT-2",
        "model_path": "text/machine_comprehension/gpt-2/model/gpt2-10.onnx",
        "onnx_version": "1.6",
        "opset_version": 10,
        "metadata": {
            "model_sha": "a4b41071a9fd8fd5591d74d6c7e3a850feaf161f7289148a3b845c84f75c55e0",
            "model_bytes": 548227537,
            "tags": [
                "text",
                "machine comprehension",
                "gpt-2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            "input1_dynamic_axes_1",
                            "input1_dynamic_axes_2",
                            "input1_dynamic_axes_3"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            1,
                            8,
                            768
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output2",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output3",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output4",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output5",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output6",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output7",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output8",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output9",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output10",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output11",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output12",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output13",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/gpt-2/model/gpt2-10.tar.gz",
            "model_with_data_sha": "0c9b81350c95cf2f40e8e72f76792ef6b3c73894beee571171739079330aad53",
            "model_with_data_bytes": 463131530
        }
    },
    {
        "model": "GPT-2-LM-HEAD",
        "model_path": "text/machine_comprehension/gpt-2/model/gpt2-lm-head-10.onnx",
        "onnx_version": "1.6",
        "opset_version": 10,
        "metadata": {
            "model_sha": "12fbb1ec2d2d70c8ebd21a290a348a4109447b98582af64c6f93b6526d5c8f35",
            "model_bytes": 664871060,
            "tags": [
                "text",
                "machine comprehension",
                "gpt-2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            "input1_dynamic_axes_1",
                            "input1_dynamic_axes_2",
                            "input1_dynamic_axes_3"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            "input1_dynamic_axes_1",
                            "input1_dynamic_axes_2",
                            "input1_dynamic_axes_3",
                            50257
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output2",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output3",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output4",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output5",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output6",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output7",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output8",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output9",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output10",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output11",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output12",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output13",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/gpt-2/model/gpt2-lm-head-10.tar.gz",
            "model_with_data_sha": "820f53e5403bb9fa0dac10af138cedcb71f282a527f04d76e35db5b2b88871c8",
            "model_with_data_bytes": 606542743
        }
    },
    {
        "model": "RoBERTa-BASE",
        "model_path": "text/machine_comprehension/roberta/model/roberta-base-11.onnx",
        "onnx_version": "1.6",
        "opset_version": 11,
        "metadata": {
            "model_sha": "ad476a33a4b227f6e6b2e1c7192df1b61640657f26b390857ab943de66236c0b",
            "model_bytes": 498649858,
            "tags": [
                "text",
                "machine comprehension",
                "roberta"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_ids",
                        "shape": [
                            "batch_size",
                            "seq_len"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output_1",
                        "shape": [
                            "batch_size",
                            "seq_len",
                            768
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output_2",
                        "shape": [
                            "batch_size",
                            768
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/roberta/model/roberta-base-11.tar.gz",
            "model_with_data_sha": "b14c5c6d6f656192390f7e7b22da2b47755f9f0a5a2b1907205096f7bf14a72d",
            "model_with_data_bytes": 295338327
        }
    },
    {
        "model": "RoBERTa-SequenceClassification",
        "model_path": "text/machine_comprehension/roberta/model/roberta-sequence-classification-9.onnx",
        "onnx_version": "1.6",
        "opset_version": 9,
        "metadata": {
            "model_sha": "8b28f588889de84ef45918765b25bd0e74179b9be5d9320dafdddcb2553784cc",
            "model_bytes": 498658080,
            "tags": [
                "text",
                "machine comprehension",
                "roberta"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch_size",
                            "sentence_length"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            "batch_size",
                            2
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/roberta/model/roberta-sequence-classification-9.tar.gz",
            "model_with_data_sha": "149f8aa33713d9ae0411cee5cdc47107bafe771e93f590de6c082b45b2f6984d",
            "model_with_data_bytes": 431685676
        }
    },
    {
        "model": "T5-encoder",
        "model_path": "text/machine_comprehension/t5/model/t5-encoder-12.onnx",
        "onnx_version": "1.6",
        "opset_version": 12,
        "metadata": {
            "model_sha": "4523122d7cf0f50905694d84995633c8a0cc223da762d3eb2aaffa17251a6f60",
            "model_bytes": 438549611,
            "tags": [
                "text",
                "machine comprehension",
                "t5"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_ids",
                        "shape": [
                            "batch",
                            "sequence"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "hidden_states",
                        "shape": [
                            "batch",
                            "sequence",
                            768
                        ],
                        "type": "tensor(float)"
                    }
                ]
            }
        }
    },
    {
        "model": "T5-decoder-with-lm-head",
        "model_path": "text/machine_comprehension/t5/model/t5-decoder-with-lm-head-12.onnx",
        "onnx_version": "1.6",
        "opset_version": 12,
        "metadata": {
            "model_sha": "235afca35d3c47a5fd7209844ac11f3157fe7263fd074197a45b7f536e40ea56",
            "model_bytes": 650564941,
            "tags": [
                "text",
                "machine comprehension",
                "t5"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_ids",
                        "shape": [
                            "batch",
                            "sequence"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "encoder_hidden_states",
                        "shape": [
                            "batch",
                            "sequence",
                            768
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "hidden_states",
                        "shape": [
                            "batch",
                            "sequence",
                            32128
                        ],
                        "type": "tensor(float)"
                    }
                ]
            }
        }
    },
    {
        "model": "LResNet100E-IR",
        "model_path": "vision/body_analysis/arcface/model/arcfaceresnet100-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "f0a2e278b430372d308fef67c1aea308c2baf37f32e8908d9bfce035c26a3fb4",
            "model_bytes": 261036388,
            "tags": [
                "vision",
                "body analysis",
                "arcface"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            112,
                            112
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc1",
                        "shape": [
                            1,
                            512
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/body_analysis/arcface/model/arcfaceresnet100-8.tar.gz",
            "model_with_data_sha": "e014dc4b9d5b512543267ae8e880bc49ac338ae9da4025c83facbaf7bf0398e0",
            "model_with_data_bytes": 237568694
        }
    },
    {
        "model": "Emotion FERPlus",
        "model_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-2.onnx",
        "onnx_version": "1.0",
        "opset_version": 2,
        "metadata": {
            "model_sha": "16f87fbec2f6b5cc1d84f90a16e9874493eb06a685104386cc79cc5ef1c46e75",
            "model_bytes": 35041945,
            "tags": [
                "vision",
                "body analysis",
                "emotion ferplus"
            ],
            "model_with_data_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-2.tar.gz",
            "model_with_data_sha": "07dd46eb1b83213f6fb743de869e60fdefe7f486a4d308f7612b67b046b3b013",
            "model_with_data_bytes": 32380999
        }
    },
    {
        "model": "Emotion FERPlus",
        "model_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "f800ecfe8a41e6ebf675f5830146e1d0c5c491cd2292a3ef3494c901963d3e52",
            "model_bytes": 35040571,
            "tags": [
                "vision",
                "body analysis",
                "emotion ferplus"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            64,
                            64
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus692_Output_0",
                        "shape": [
                            1,
                            8
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-7.tar.gz",
            "model_with_data_sha": "a20af13561752fffb11fc250e1f0e6d31e2d448242e69186700d23fdff6ec0e7",
            "model_with_data_bytes": 32384474
        }
    },
    {
        "model": "Emotion FERPlus",
        "model_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "a2a2ba6a335a3b29c21acb6272f962bd3d47f84952aaffa03b60986e04efa61c",
            "model_bytes": 35040571,
            "tags": [
                "vision",
                "body analysis",
                "emotion ferplus"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            64,
                            64
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus692_Output_0",
                        "shape": [
                            1,
                            8
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-8.tar.gz",
            "model_with_data_sha": "2941173471d94d9b9592260573c4d313ca3c312a20fd40b90f9c01886d6e08d6",
            "model_with_data_bytes": 32384491
        }
    },
    {
        "model": "version-RFB-320",
        "model_path": "vision/body_analysis/ultraface/models/version-RFB-320.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "34cd7e60aeff28744c657de7a3dc64e872d506741de66987f3426f2b79f88017",
            "model_bytes": 1270727,
            "tags": [
                "vision",
                "body analysis",
                "ultraface"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            240,
                            320
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "scores",
                        "shape": [
                            1,
                            4420,
                            2
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "boxes",
                        "shape": [
                            1,
                            4420,
                            4
                        ],
                        "type": "tensor(float)"
                    }
                ]
            }
        }
    },
    {
        "model": "version-RFB-640",
        "model_path": "vision/body_analysis/ultraface/models/version-RFB-640.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "8f4c659275977e7a3bfbfa339a9c769ad793df50f9c0baa8c14b11baa1646430",
            "model_bytes": 1588012,
            "tags": [
                "vision",
                "body analysis",
                "ultraface"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            480,
                            640
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "scores",
                        "shape": [
                            1,
                            17640,
                            2
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "boxes",
                        "shape": [
                            1,
                            17640,
                            4
                        ],
                        "type": "tensor(float)"
                    }
                ]
            }
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "e9e222ef55ba873b219f22987130fb0567cb7bc7d3f1afeb47ff442bc0e11b6d",
            "model_bytes": 243863514,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-3.tar.gz",
            "model_with_data_sha": "b640cc797d25aa446cebefaa3274d517f0c25e26d66eb6f6686ac7e4d4d8d584",
            "model_with_data_bytes": 229606515
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "f1ae53b93d83dd74731c84337ae64035288ce336794a46e2b17b56e7566feaab",
            "model_bytes": 243863628,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-6.tar.gz",
            "model_with_data_sha": "7afb4036281dfbcb33edec2fc3928cc9da5e015aee9142c03faa81b3f8481db3",
            "model_with_data_bytes": 229606527
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "d9c9134b90c760d4231e89ad9758bcc8a9397c3ba1cd56fadbef888b534ac6b4",
            "model_bytes": 243863542,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-7.tar.gz",
            "model_with_data_sha": "d9bb9bf70d9591a20b7b213c7e6daafe5de257acd987592d3f4ef000bdc979fd",
            "model_with_data_bytes": 231241493
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "9a4dc22fd706a19462aaec3b08e027e0ef89daf63d123824f20b59b6de9a727b",
            "model_bytes": 243863542,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-8.tar.gz",
            "model_with_data_sha": "7d056c881ba90d83cff23cb04d1af51d3beed50ec06a432865b1267be6629973",
            "model_with_data_bytes": 231241418
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "ef3c82dad5c512f969dbf701fe35c0b3feb1585687d541f765eb68730c0a8af7",
            "model_bytes": 243863542,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-9.tar.gz",
            "model_with_data_sha": "bcc6d092a2a50fb1c114854f8e22487750a2a09e27eaa9351bdc58f637ee3e1a",
            "model_with_data_bytes": 231241421
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "e362cbd353e7a0432d46ec608bfc38646aa8d70207ddb92788008816603ce835",
            "model_bytes": 243863525,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-3.tar.gz",
            "model_with_data_sha": "d6a210e64d1964f8132cbe9ea46d4ab8e7fcf2634ce4285b701deb3fe3e7dd1c",
            "model_with_data_bytes": 229550734
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "15e568474e1c2af94cc34b812628707fee0130f3a3c8b3329cbc5e4ba9248682",
            "model_bytes": 243863639,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-6.tar.gz",
            "model_with_data_sha": "bf91013f28e461722dc47acfc1ebd8464c8320d4276902204a1d810a6dd2ff2a",
            "model_with_data_bytes": 229550795
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "a249fe5683aa6b517dfb198e4b3bd40a1b7611d4995fde0b0b9ced5f4143bc5c",
            "model_bytes": 243863553,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-7.tar.gz",
            "model_with_data_sha": "e0287e08e40dcaf179e450da00da75b467235da6ded9f8bdf27b37ef15c340e1",
            "model_with_data_bytes": 229551315
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "82f8997778cd9c5ef9673fc86e340f86e3b29650886b923e5e1aa478d5f83f92",
            "model_bytes": 243863553,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-8.tar.gz",
            "model_with_data_sha": "69292692098ccecb7df067c577165de0c2c1885cca821045648f93f6964af0ed",
            "model_with_data_bytes": 229551319
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "802138332cf3e423c82a9a80c76e4f9227dd4effb3aba55413601fa4345cfd36",
            "model_bytes": 243863553,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-9.tar.gz",
            "model_with_data_sha": "ae8ac99c5627bc69e63db078a4fb46b547e1a589010eb0bd97dd1501646dea14",
            "model_with_data_bytes": 229551368
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "7749c7b6c419cf2b8c6b8a979c86bb3d322976578682662b852cae36e2b79b41",
            "model_bytes": 32718955,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-3.tar.gz",
            "model_with_data_sha": "70586d81f49ad3bbe733270a1a58b5235794d51da3f9302cbc1e634accb6113d",
            "model_with_data_bytes": 33658961
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "875fb597a9f0f79cca8a9db610a355920b13959fbe7dc35b759dd35f4d25e291",
            "model_bytes": 32719461,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-6.tar.gz",
            "model_with_data_sha": "68377f0df580eb3a269d6afd5e3837ea75de23a28ae11e88c4714213b1df44d7",
            "model_with_data_bytes": 33660618
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "79c3bc0974696a2ea9efd2f7bca19fdd630834bd0086f1b4a1c3db3dce3b2a51",
            "model_bytes": 32719461,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-7.tar.gz",
            "model_with_data_sha": "51962687b5fd5ac636fb8b682a463d607f5c6713c21514fcd7d89a02c81a3d11",
            "model_with_data_bytes": 33660614
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "c57fd1e5864ddc808a84567d0d3a1ea6b5e4d1afbec17372723503be7a030e3a",
            "model_bytes": 32719461,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-8.tar.gz",
            "model_with_data_sha": "c44501ca60c207076bd400f2dbedcfc19b92e98a4fffe929a4e007acccb207c0",
            "model_with_data_bytes": 33660609
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "875fb597a9f0f79cca8a9db610a355920b13959fbe7dc35b759dd35f4d25e291",
            "model_bytes": 32719461,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-9.tar.gz",
            "model_with_data_sha": "68377f0df580eb3a269d6afd5e3837ea75de23a28ae11e88c4714213b1df44d7",
            "model_with_data_bytes": 33660618
        }
    },
    {
        "model": "EfficientNet-Lite4",
        "model_path": "vision/classification/efficientnet-lite4/model/efficientnet-lite4-11.onnx",
        "onnx_version": "1.7.0",
        "opset_version": 11,
        "metadata": {
            "model_sha": "d111689907c06eea7c82e4833ddef758da6453b9d4cf60b7e99ca05c7cbd9c12",
            "model_bytes": 51946641,
            "tags": [
                "vision",
                "classification",
                "efficientnet-lite4"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "images:0",
                        "shape": [
                            1,
                            224,
                            224,
                            3
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Softmax:0",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/efficientnet-lite4/model/efficientnet-lite4-11.tar.gz",
            "model_with_data_sha": "1914de663a575f0ec015b7a33c335b363e910f64cc91d08e66587ea877fd9d7b",
            "model_with_data_bytes": 48592826
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "5725ccfd90845735ce5897faf7843c3f9b13acda3c4f3bff82cb21c9337c9de8",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-3.tar.gz",
            "model_with_data_sha": "3e4045f1cfec9936588a1f0352ce32b4764cc844013403f8116854c5bc4ff184",
            "model_with_data_bytes": 30906583
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "bd979d99d10d9acf84c5a67252bcae4992bf4e1afdab046cb0ea370e6f01ee38",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-6.tar.gz",
            "model_with_data_sha": "d8e2cf0321f97283d3e8664e0e35fffc5aee52c4c078d0655b29e4972867bcac",
            "model_with_data_bytes": 30906642
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "5725ccfd90845735ce5897faf7843c3f9b13acda3c4f3bff82cb21c9337c9de8",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-7.tar.gz",
            "model_with_data_sha": "3e4045f1cfec9936588a1f0352ce32b4764cc844013403f8116854c5bc4ff184",
            "model_with_data_bytes": 30906583
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "3d343d459199fbcc8cd43d45264180e3e1ef91d4908f3541b5b308995206362c",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-8.tar.gz",
            "model_with_data_sha": "24f8576ba8ea7e0589af6dabfc236e564dd01d94823a2fcb6a762ad3bf736c37",
            "model_with_data_bytes": 30906601
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "bd979d99d10d9acf84c5a67252bcae4992bf4e1afdab046cb0ea370e6f01ee38",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-9.tar.gz",
            "model_with_data_sha": "d8e2cf0321f97283d3e8664e0e35fffc5aee52c4c078d0655b29e4972867bcac",
            "model_with_data_bytes": 30906642
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "6e6a6091a76dcc34b7afd68cec9355687278844b92fb0364b608e5c6f33067e8",
            "model_bytes": 28020217,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-3.tar.gz",
            "model_with_data_sha": "18dd953ae8f2056ac0a1fd56302ef2a5d56b35ccee44fddc7bbdcbcfe9a5adf1",
            "model_with_data_bytes": 29324028
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "9e7c4e026430fc055cb040dfe7f7978556b3af13cd618a7866841c00dd6d178f",
            "model_bytes": 28020390,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-6.tar.gz",
            "model_with_data_sha": "caf55082c6605ff17e0074e7af862f774b6efb00557236676e7112f6f54cec61",
            "model_with_data_bytes": 29324092
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "eaa5c289961ef2134290ddd42852d818d1d40fce18ffbc876bc446483343af9c",
            "model_bytes": 28020356,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-7.tar.gz",
            "model_with_data_sha": "7bae43d53cc836bcd20cc5eacdd12c2d24d34c17e960a3342d7501489d5d9157",
            "model_with_data_bytes": 29323411
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "ec956d393dd467b6d9e0b17b8c8227c534b936bfbcdc82343a3a7238375775f0",
            "model_bytes": 28020356,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-8.tar.gz",
            "model_with_data_sha": "2104acf056bf9c7ce2407fd1e886eb9bec78413639c7c14f1c15a096305932b7",
            "model_with_data_bytes": 29323364
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "f8b7bb4040377b83089eef98800a15303325ed75ced468a8972a87e0bc23692e",
            "model_bytes": 28020356,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-9.tar.gz",
            "model_with_data_sha": "2242cbd72b324f384e767634bc9acf4db240aea3ae15d6a1817ee81256d016ce",
            "model_with_data_bytes": 29323358
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "8bb3ba787e84667bc1169034c36a1f1c1cddfbd19d5e48c9c51306a087973bc8",
            "model_bytes": 45042595,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-3.tar.gz",
            "model_with_data_sha": "37ea826b395c559ed2cc148823ea77f793e591c5eff5a718d98733793b680dbc",
            "model_with_data_bytes": 45015362
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "53626ce95a96756469d23b8dd917f8b5a6015c628369d219bdbf17bb5d0178f5",
            "model_bytes": 45040501,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-6.tar.gz",
            "model_with_data_sha": "a29498bbf22d78dc428f0b6a0ecdc350475896ccb653c0b8d544c14000509b0a",
            "model_with_data_bytes": 45015338
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "e1ada57696a4d2c23984b8584cd8df40243301f7344475a8653118c0d5da57a5",
            "model_bytes": 45042799,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-7.tar.gz",
            "model_with_data_sha": "c23f2596676c27640d668115588c25771a5645966e5f00e858295e3cd9fbfd58",
            "model_with_data_bytes": 45015328
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "878e9c04172215cb7a1ce225dfecfe2ed098abde5cb1067dc644a3629ec6dcf1",
            "model_bytes": 45042799,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-8.tar.gz",
            "model_with_data_sha": "b2e7863a96ace0783328586e75fd59ccaa7e7de7bd72be6d030cc206e312b690",
            "model_with_data_bytes": 45015310
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "37ed4aa93f817fd0d1cc19bda045ebd70f18d0887a5f51e66ecd0ed8fd5a65c2",
            "model_bytes": 45042799,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-9.tar.gz",
            "model_with_data_sha": "e4b06f72212011ed60e3515242ea7f7695ff59e0735633953573ad053b57abfe",
            "model_with_data_bytes": 45015320
        }
    },
    {
        "model": "MNIST",
        "model_path": "vision/classification/mnist/model/mnist-1.onnx",
        "onnx_version": "1.0",
        "opset_version": 1,
        "metadata": {
            "model_sha": "22239f3fcc38f34d02eecd6869aed15b93f8e3e1125dda48990d244a5e113d49",
            "model_bytes": 27266,
            "tags": [
                "vision",
                "classification",
                "mnist"
            ],
            "model_with_data_path": "vision/classification/mnist/model/mnist-1.tar.gz",
            "model_with_data_sha": "a296485a657137a78b5c9b0ade09fca4041364da2c530dd0edfca1e1bb53283e",
            "model_with_data_bytes": 26518
        }
    },
    {
        "model": "MNIST",
        "model_path": "vision/classification/mnist/model/mnist-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "0d715376572e89832685c56a65ef1391f5f0b7dd31d61050c91ff3ecab16c032",
            "model_bytes": 26454,
            "tags": [
                "vision",
                "classification",
                "mnist"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus214_Output_0",
                        "shape": [
                            1,
                            10
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mnist/model/mnist-7.tar.gz",
            "model_with_data_sha": "89edba3ef306f620acece85ec7bf4629a3f58c597a2b9cce9fc64ab8b1e9b452",
            "model_with_data_bytes": 25959
        }
    },
    {
        "model": "MNIST",
        "model_path": "vision/classification/mnist/model/mnist-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "2f06e72de813a8635c9bc0397ac447a601bdbfa7df4bebc278723b958831c9bf",
            "model_bytes": 26454,
            "tags": [
                "vision",
                "classification",
                "mnist"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus214_Output_0",
                        "shape": [
                            1,
                            10
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mnist/model/mnist-8.tar.gz",
            "model_with_data_sha": "1dd098b0fe8bc750585eefc02013c37be1a1cae2bdba0191ccdb8e8518b3a882",
            "model_with_data_bytes": 25962
        }
    },
    {
        "model": "MobileNet v2-1.0",
        "model_path": "vision/classification/mobilenet/model/mobilenetv2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "0e7c0aa4bc74650386fa1d2c84705753de7c2bdb21909ada5c59154bb429e092",
            "model_bytes": 13963115,
            "tags": [
                "vision",
                "classification",
                "mobilenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch_size",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            "batch_size",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mobilenet/model/mobilenetv2-7.tar.gz",
            "model_with_data_sha": "147af1703264c68d6e0880b3f807bf43fd5ada98d2df6b4c4aec3be6ac26d6f2",
            "model_with_data_bytes": 14570532
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "bc0f63a5f40bad37092c456cac821a99ee63a0f5a453f045e4b39d25823ddbdb",
            "model_bytes": 230753133,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-3.tar.gz",
            "model_with_data_sha": "eb4fab60b84e2c69dc634f91bd669867ac7ef8828499a28023472df3219ac59c",
            "model_with_data_bytes": 217332838
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "d6bbbd835d0e0654b0fceda3b186505a4d89fd8cf0282f403035823d0bdefa8f",
            "model_bytes": 230753247,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-6.tar.gz",
            "model_with_data_sha": "399c2629de63824d52b5f71db731995492f4ebb8a2892fa36236925a4bad9f44",
            "model_with_data_bytes": 217332967
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "46cf29e309bb65b9f754839aa214f3c13caddccd3204178d9034ce091c4bfd17",
            "model_bytes": 230753161,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc-rcnn_1",
                        "shape": [
                            1,
                            200
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-7.tar.gz",
            "model_with_data_sha": "527a6470798ce06425202dc002df041c49b7cab55c24406ac13b3d47bc44d0ea",
            "model_with_data_bytes": 218957602
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "eedeefa170e2d541c6bbd60a935f79396f56721aef1cfdfe79e767ed536deb85",
            "model_bytes": 230753161,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc-rcnn_1",
                        "shape": [
                            1,
                            200
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-8.tar.gz",
            "model_with_data_sha": "b10a14df9ab26260838a1b306c366efd1ede2eed81768888a231148479e5f67a",
            "model_with_data_bytes": 218957576
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "4aadb5ac806e82f89bac7dea77c838ba828dccbe5d624d8a44ebd513953571d9",
            "model_bytes": 230753161,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc-rcnn_1",
                        "shape": [
                            1,
                            200
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-9.tar.gz",
            "model_with_data_sha": "f3f300570508439f538f7ae7325b1b121b69fd539a775087c59e0fcc4d363951",
            "model_with_data_bytes": 218957598
        }
    },
    {
        "model": "ResNet18",
        "model_path": "vision/classification/resnet/model/resnet18-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "433c77f0a2e1394ca452f86b097f50d78e4888a9d4860348688d1b0bb9acc292",
            "model_bytes": 46820735,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv15_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet18-v1-7.tar.gz",
            "model_with_data_sha": "9f44e036690712d57ce07cfc433b7eed32edbe9fe63c02d4c8b0c6f6f06b4f4d",
            "model_with_data_bytes": 44992229
        }
    },
    {
        "model": "ResNet34",
        "model_path": "vision/classification/resnet/model/resnet34-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "495e37f161ef24920b3e1883faa22eb87907829ac05120acca07cd7e9e383e7c",
            "model_bytes": 87302586,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv16_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet34-v1-7.tar.gz",
            "model_with_data_sha": "08547c6d76297de11775db27d598e4ed902f94649e281f7e6599be082cc12a71",
            "model_with_data_bytes": 82449233
        }
    },
    {
        "model": "ResNet50",
        "model_path": "vision/classification/resnet/model/resnet50-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "3efe394824ceb1bc354bc08c4fa477f60a4505cf4259e42bda3ac90ad2aa6af6",
            "model_bytes": 102583338,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv17_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-v1-7.tar.gz",
            "model_with_data_sha": "c69e9cb3e709ca7e6ec97a334bdc7e91eda58db77f3d93b1d814539cf6263fc4",
            "model_with_data_bytes": 96686465
        }
    },
    {
        "model": "ResNet101",
        "model_path": "vision/classification/resnet/model/resnet101-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "faafaf3aea406a105e26805cf3e8e591549e1c06feb60a8e60d85bcf94f8d8c5",
            "model_bytes": 178914041,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv18_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet101-v1-7.tar.gz",
            "model_with_data_sha": "e6c1914c942be3305d9caea84bb03a82193bcad00052aeeda5965b9a8c1ebb08",
            "model_with_data_bytes": 167594279
        }
    },
    {
        "model": "ResNet152",
        "model_path": "vision/classification/resnet/model/resnet152-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "a1c165290f7ac03128a24d9e34e8497d27630498d503c80b762a41169b64c112",
            "model_bytes": 241816204,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv19_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet152-v1-7.tar.gz",
            "model_with_data_sha": "cb9927d5c9b361ceda38d0c6896936982199b2d026af0399f6a4d55a42f95a43",
            "model_with_data_bytes": 227763406
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "59d7b26391117eb7968feb384be692c1b80619330f5d4cfe25c4efc5f7e4a12e",
            "model_bytes": 102491871,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-3.tar.gz",
            "model_with_data_sha": "074ba053b6aa46c6b74d09ba9b742335e59d5220951f576dbe1fd40abe3b6601",
            "model_with_data_bytes": 96826419
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "129da56da1667008f911e79ac9619cd39fef0fd34d0102835e25007cc3044240",
            "model_bytes": 102490289,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-6.tar.gz",
            "model_with_data_sha": "89e669a4c7d5a561cc148f962f7ae2c406ae2338185caec6ab8f9647277f782a",
            "model_with_data_bytes": 98452623
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "75f14934cf138fc4912e3a8d33315d8065b01172eacbe65db4e6eb2f770f7a91",
            "model_bytes": 102489423,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-7.tar.gz",
            "model_with_data_sha": "ec5da15009b3a150508c01bb54872367f74f24faf3e3a04c71a2ded1f0bd2ed3",
            "model_with_data_bytes": 101706350
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "d52ce8f6349001e14226e354bb815aba5b35112b0e8837a884448d06e667853d",
            "model_bytes": 102489423,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-8.tar.gz",
            "model_with_data_sha": "3700e4f398267402a565c54c15bd476bef2fb34212eebf9bb6fe024fdf64379a",
            "model_with_data_bytes": 101706397
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "78eecdb9354e71364b9df6f3b5824ecc48710938d5b4ea23724b9a2e9edbc4a6",
            "model_bytes": 102489423,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-9.tar.gz",
            "model_with_data_sha": "ac8d0674c0e919fdd995c4068894381e9b2bb0313fd67bfffc790bcc0f31d38c",
            "model_with_data_bytes": 101706363
        }
    },
    {
        "model": "ResNet18",
        "model_path": "vision/classification/resnet/model/resnet18-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "bd75e414be9d14d50c8c067121e90a7bec0531a4793fe21e5dcb6bc66f2addcb",
            "model_bytes": 46806735,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv22_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet18-v2-7.tar.gz",
            "model_with_data_sha": "b94e8dd1543f36149ade0767232398f9e7eb4921047f4ae7c261a32cf0ecc659",
            "model_with_data_bytes": 44966711
        }
    },
    {
        "model": "ResNet34",
        "model_path": "vision/classification/resnet/model/resnet34-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "17f9350e613339ffd46314bb905998f1adaaa6cf983a8c058efbb3cbc6046ca5",
            "model_bytes": 87288585,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv23_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet34-v2-7.tar.gz",
            "model_with_data_sha": "c333b01edeee5bdb9e3a3b6acc76968bfb3a50ea36b80976aac26a98afd28848",
            "model_with_data_bytes": 82421588
        }
    },
    {
        "model": "ResNet50",
        "model_path": "vision/classification/resnet/model/resnet50-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "1e80e8a7c8eec8e8b20c440d7dd7f2f8590779c7c2859dc4514870dc463845d3",
            "model_bytes": 102442450,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv24_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-v2-7.tar.gz",
            "model_with_data_sha": "0a52a62f690ffaeea08a10673bf77de7c4ad2b2f3dad638156c5a922e679f679",
            "model_with_data_bytes": 96494594
        }
    },
    {
        "model": "ResNet101",
        "model_path": "vision/classification/resnet/model/resnet101-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "b610c141f3b26bf8324dcbd13532779cceedffac5213dbcc3ad0047b489ce304",
            "model_bytes": 178682299,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv25_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet101-v2-7.tar.gz",
            "model_with_data_sha": "ac8363b02d0f8fc911c0cd5b9c74dbf1a0b94d7f29d0a608b8977c9eea81fe59",
            "model_with_data_bytes": 167263639
        }
    },
    {
        "model": "ResNet152",
        "model_path": "vision/classification/resnet/model/resnet152-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "12ee6c5da7ec0437b5405d51ab459a4980ed91d51b0cd7e3c0a0b276ad0b1caa",
            "model_bytes": 241503846,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv27_dense0_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet152-v2-7.tar.gz",
            "model_with_data_sha": "f511fb360f7b09371232b246215101a286cb5977d6689cd258201650a7468bd7",
            "model_with_data_bytes": 226621705
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "ebbf0f6cf45d6971dee7833bfa9cceb868f19ed753c5bf6053857cdb68fc15bc",
            "model_bytes": 5723516,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-3.tar.gz",
            "model_with_data_sha": "eeeef9e70d858a3fd72b9eb33435e480a9f2fac0fa0f2d2c75b148fe3e62cb7f",
            "model_with_data_bytes": 6980244
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "faabc409fd57ae83ddebe7fb981ee492a7899c1acb4893cc8e64c11f0ccdaa98",
            "model_bytes": 5723770,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-6.tar.gz",
            "model_with_data_sha": "33e0e2976a7e319aa7e5767768b6324bc3c037ea6b15ec7695f9959f7df12d70",
            "model_with_data_bytes": 8604891
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "359609ab27d27d44091e05f9a5127d9ce0c155ed69d0716f41e6779998bc4764",
            "model_bytes": 5723770,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-7.tar.gz",
            "model_with_data_sha": "2374f4c1808e0fc4a9743849f82fa617f2681b404b76611f3df4c23534edc88e",
            "model_with_data_bytes": 8604888
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "b0e5f38aa3961e6d208ccfe5e34dc52c1b9bf96f4b377cdba1919c6810457258",
            "model_bytes": 5724572,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-8.tar.gz",
            "model_with_data_sha": "9f1c78a572dcf40f1056bdb9688433290c010c91e98ba4526be1ffdbfd854660",
            "model_with_data_bytes": 8604055
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "3642c54024f09d35236f325df9822e772596c394cd90fd8e98ca1d2eba309e6f",
            "model_bytes": 5723770,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-9.tar.gz",
            "model_with_data_sha": "2f71595f1ca1aa5ac6d062716c0be4f451fdaf301cd68e82c68aaaf1e006e126",
            "model_with_data_bytes": 8604895
        }
    },
    {
        "model": "ShuffleNet-v2",
        "model_path": "vision/classification/shufflenet/model/shufflenet-v2-10.onnx",
        "onnx_version": "1.6",
        "opset_version": 10,
        "metadata": {
            "model_sha": "8b85e969a0a5518b554c99548379d1ba54d973733a6a12012138d91852308584",
            "model_bytes": 9218554,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-v2-10.tar.gz",
            "model_with_data_sha": "3dfa8224c1233b22006b43c689eebc4af46917cc1de0bd123f43510d8ab06bf2",
            "model_with_data_bytes": 8721925
        }
    },
    {
        "model": "SqueezeNet 1.1",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "1eeff551a67ae8d565ca33b572fc4b66e3ef357b0eb2863bb9ff47a918cc4088",
            "model_bytes": 4956208,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "squeezenet0_flatten0_reshape0",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.1-7.tar.gz",
            "model_with_data_sha": "aff6280d73c0b826f088f7289e4495f01f6e84ce75507279e1b2a01590427723",
            "model_with_data_bytes": 6231548
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "f68e6ad41574fa495fc28db95dd47b78a79522e305990f6f6d2a1844607f886d",
            "model_bytes": 4952238,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-3.tar.gz",
            "model_with_data_sha": "80dc32f8172209d139160258da093dc08af95e61ec4457f98b9061499020331c",
            "model_with_data_bytes": 6226586
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "513a21407cccfcc74eb20e7749de7d5dda38f3f3199fd3259deeca9c021d94a0",
            "model_bytes": 4952238,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-6.tar.gz",
            "model_with_data_sha": "9da41b593f063b70d41d1a8245983ce85ba4fe6f2402225e9e04e19fd943173a",
            "model_with_data_bytes": 6226584
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "d95e2191e056f1912a9b8f6000da3b9c7818441b9eb48137033c2adbf6398bc8",
            "model_bytes": 4952222,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-7.tar.gz",
            "model_with_data_sha": "ddb50fe7961120427b5120506b82936e90faaaa4214d74be3234b943fb73cbbd",
            "model_with_data_bytes": 11131117
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "3c0bff2f7c984350d3ffa98e0167dc08c15da88f05d0c31989ebeba6fbb5e025",
            "model_bytes": 4952222,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-8.tar.gz",
            "model_with_data_sha": "84b5548e9c1610ccd330df97d83922e7402751ba9ab0a60f559daca1ea715f02",
            "model_with_data_bytes": 11131147
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "f2d3201f013b72988109e677e9244250f4cc02394529432972f05e1f6dcc7b62",
            "model_bytes": 4952222,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-9.tar.gz",
            "model_with_data_sha": "f4c9a2906a949f089bee5ef1bf9ea1c0dc1b49d5abeb1874fff3d206751d0f3b",
            "model_with_data_bytes": 11131125
        }
    },
    {
        "model": "VGG 16",
        "model_path": "vision/classification/vgg/model/vgg16-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "f20805a3ecccaa88647bbab4ad011ff2412a9838485ea844de0fcbce349820b9",
            "model_bytes": 553437328,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg16-7.tar.gz",
            "model_with_data_sha": "67ae525d46b971bde45c7e5830c50f09ce4c7cdc97bf6dce48a4238a9b69efba",
            "model_with_data_bytes": 513753139
        }
    },
    {
        "model": "VGG 16-bn",
        "model_path": "vision/classification/vgg/model/vgg16-bn-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "4fdceabf1ef715c9ed3020fa2eac1cb2a448296c086b03633c80cf47e1c5afce",
            "model_bytes": 553512191,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg16-bn-7.tar.gz",
            "model_with_data_sha": "038c2009704127b644a613de6bb0c69152663c96e5085dfae8440ed37c512b65",
            "model_with_data_bytes": 513970920
        }
    },
    {
        "model": "VGG 19",
        "model_path": "vision/classification/vgg/model/vgg19-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "692f9f927f1df1b89033787a77a73eac59fd9b3344f9ecdf0b13221e41c2046f",
            "model_bytes": 574677321,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-7.tar.gz",
            "model_with_data_sha": "b484c34c59b294d80aff6de07769b1077275915cf761eb3eeaa81c8f502d5e2b",
            "model_with_data_bytes": 533203669
        }
    },
    {
        "model": "VGG 19-bn",
        "model_path": "vision/classification/vgg/model/vgg19-bn-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "ce0987dfa6228ad29708013f38b7f73cbc14ca6066a05fc22ee49b35871c63ed",
            "model_bytes": 574774380,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-bn-7.tar.gz",
            "model_with_data_sha": "7578a4037b5121ded13fe413984410e020fbc1a380448262832ec3fd0aa3864d",
            "model_with_data_bytes": 533350067
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "2a7d16457887670e978c9bcfc64e686482d1da807fcfbd7536074e47ad4540f7",
            "model_bytes": 574674684,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-3.tar.gz",
            "model_with_data_sha": "8646cc6d0252d3453328da9a15a32e82201bbf6854a865125344e2430c90a4e1",
            "model_with_data_bytes": 536839176
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "088ee597d84957594ded3fd44d669ed142ce2484048e1d9f7fafeb66d12913fa",
            "model_bytes": 574674798,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-6.tar.gz",
            "model_with_data_sha": "289f11496aaa99ca1fe68ea0912b96bfaae4dcc55263187dfdc844e6f5022522",
            "model_with_data_bytes": 536839354
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "967f4f585137782d541301ba2d4a9c006be92671de53e37739d92e386e3124ea",
            "model_bytes": 574674712,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-7.tar.gz",
            "model_with_data_sha": "0d86de02fcd018ca9ef474fa28f1846bcbe88c149c8920afde43abba30696284",
            "model_with_data_bytes": 536839305
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "639ce28961555254d0726c6bb7b2ae7bb9665233d00e04d1add3425aae86cb87",
            "model_bytes": 574674712,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-8.tar.gz",
            "model_with_data_sha": "033f76e8bd7b8473d1ff28cc2536d13d03c86f0244703c96268a5bf49d0a9efa",
            "model_with_data_bytes": 536839273
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "cfb97260b303ad3963999662456571b89c5f73dbdd2fd9dffd19fed7a90f766b",
            "model_bytes": 574674712,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-9.tar.gz",
            "model_with_data_sha": "7f2aa43c7809f0ad9d4243e076f3da8a1b125de82fc1d1b69dbfb717209cfff6",
            "model_with_data_bytes": 536839233
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "7a30178001394e21ccaac288942f4130f0099d4d3263b8ba2272f0cf2a988f49",
            "model_bytes": 349005312,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-3.tar.gz",
            "model_with_data_sha": "5a9235aba3477532a59adcb5bc9f98c7e644e8c937ab1deba28ecd774b1ebdde",
            "model_with_data_bytes": 326944060
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "80158bfe8c1191263d003bfaa62694e7cf389b4a472456e389ceadbc4dab684d",
            "model_bytes": 349005426,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-6.tar.gz",
            "model_with_data_sha": "5399c62056d5eb51d601de4280871e30b86241e00cc95a0fef59049c02f4996a",
            "model_with_data_bytes": 326892866
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "b62d3e67f480d3faa3757dba77f519008bc985072ae163ee0bbcac426916efee",
            "model_bytes": 349005372,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-7.tar.gz",
            "model_with_data_sha": "f198b1aee75f80a3d5448ad2d2f181181da23956e38d7295be7f3cdf52bcafcf",
            "model_with_data_bytes": 325254100
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "2b70b4d43c6e1b8a19b93bb465cb40c2f4f656abac13aebd1b3105e9c904b486",
            "model_bytes": 349005372,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-8.tar.gz",
            "model_with_data_sha": "cd65e8f8f5686d913b75ad1874abc9d7f9a3fb20d9eeeff87156aa125f33eb6c",
            "model_with_data_bytes": 325254018
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "55e3cb74e3d1744e0a8d9fecadd79e9f52887891e2e17df639670ac7293d99ba",
            "model_bytes": 349005372,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-9.tar.gz",
            "model_with_data_sha": "8d7c18e84eacda744e7415d68f0c427dd77c67a42a7818f0242fbdd15050d4aa",
            "model_with_data_bytes": 325254106
        }
    },
    {
        "model": "ResNet101_DUC_HDC",
        "model_path": "vision/object_detection_segmentation/duc/model/ResNet101-DUC-7.onnx",
        "onnx_version": "1.2.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "6d3b29046b6df3e993ac7fd7253a1070c8e24a0c9690604e15211f78f1efc4e4",
            "model_bytes": 260681709,
            "tags": [
                "vision",
                "object detection segmentation",
                "duc"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            800,
                            800
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "seg_loss",
                        "shape": [
                            1,
                            19,
                            160000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/duc/model/ResNet101-DUC-7.tar.gz",
            "model_with_data_sha": "6ef022454971b1362fef68234cfc63d9dd6336186b5dd48b4413032bebdd423e",
            "model_with_data_bytes": 295745909
        }
    },
    {
        "model": "Faster R-CNN R-50-FPN",
        "model_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "dfb81423efbea52e45df242ade64cfca0ba05fae78e00cf0c68a0979987f87eb",
            "model_bytes": 167330019,
            "tags": [
                "vision",
                "object detection segmentation",
                "faster-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6379",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6381",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6383",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-10.tar.gz",
            "model_with_data_sha": "b71db127152f4ee949f521feaaf74752e1638abff8a610221b0ecb746d1545d9",
            "model_with_data_bytes": 158020758
        }
    },
    {
        "model": "FCN ResNet-50",
        "model_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet50-11.onnx",
        "onnx_version": "1.8.0",
        "opset_version": 11,
        "metadata": {
            "model_sha": "8abd3ae6c258e6fd210d8fb296261ea5c0603c6cc2f568db6097f6fbaaeff0d5",
            "model_bytes": 141193553,
            "tags": [
                "vision",
                "object detection segmentation",
                "fcn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch",
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "out",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "aux",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet50-11.tar.gz",
            "model_with_data_sha": "05b951717bd910d9194f207373f36eb99d0dedb76c0f2526c83c6a70fb3edf0e",
            "model_with_data_bytes": 223950703
        }
    },
    {
        "model": "FCN ResNet-101",
        "model_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet101-11.onnx",
        "onnx_version": "1.8.0",
        "opset_version": 11,
        "metadata": {
            "model_sha": "fd63d026cf4b2e54ef2bffb766914ddbffef9bd7a67eba0e67dbf1514bcf539d",
            "model_bytes": 217069155,
            "tags": [
                "vision",
                "object detection segmentation",
                "fcn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch",
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "out",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "aux",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet101-11.tar.gz",
            "model_with_data_sha": "672ca7f736bff936f6b5fa1c8e07546f5775edc55bf33d62ae21ab372e8f6065",
            "model_with_data_bytes": 294729465
        }
    },
    {
        "model": "Mask R-CNN R-50-FPN",
        "model_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "a519d8102cb162e78cbf123615aa5a8f3bf9d0fa1dec61a2fbbb42fa3f0e0757",
            "model_bytes": 177925424,
            "tags": [
                "vision",
                "object detection segmentation",
                "mask-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6568",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6570",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6572",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6887",
                        "shape": [
                            "nbox",
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-10.tar.gz",
            "model_with_data_sha": "3101f45c6977cfa66097f4b711d3909935c4d47f97802e75ccfe66dbe77cc128",
            "model_with_data_bytes": 168800296
        }
    },
    {
        "model": "RetinaNet (ResNet101 backbone)",
        "model_path": "vision/object_detection_segmentation/retinanet/model/retinanet-9.onnx",
        "onnx_version": "1.6.0",
        "opset_version": 9,
        "metadata": {
            "model_sha": "06742923960ec4d9899e6fe407d4d2df013fe6962504f099463ca1b8cba45e44",
            "model_bytes": 228369343,
            "tags": [
                "vision",
                "object detection segmentation",
                "retinanet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            480,
                            640
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            720,
                            60,
                            80
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output2",
                        "shape": [
                            1,
                            720,
                            30,
                            40
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output3",
                        "shape": [
                            1,
                            720,
                            15,
                            20
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output4",
                        "shape": [
                            1,
                            720,
                            8,
                            10
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output5",
                        "shape": [
                            1,
                            720,
                            4,
                            5
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output6",
                        "shape": [
                            1,
                            36,
                            60,
                            80
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output7",
                        "shape": [
                            1,
                            36,
                            30,
                            40
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output8",
                        "shape": [
                            1,
                            36,
                            15,
                            20
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output9",
                        "shape": [
                            1,
                            36,
                            8,
                            10
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output10",
                        "shape": [
                            1,
                            36,
                            4,
                            5
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/retinanet/model/retinanet-9.tar.gz",
            "model_with_data_sha": "3a710eee22a7c2ae8e9d6166774d0fd7b7438146ed2d2e993b9694eb39c3addc",
            "model_with_data_bytes": 153330008
        }
    },
    {
        "model": "SSD-MobilenetV1",
        "model_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_10.onnx",
        "onnx_version": "1.7.0",
        "opset_version": 10,
        "metadata": {
            "model_sha": "1fbcf47654165f2e0b5f1bdf3f123b9e9e1128cd6463717767b76ab4b5246f9a",
            "model_bytes": 29275103,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd-mobilenetv1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image_tensor:0",
                        "shape": [
                            "unk__8520",
                            "unk__8521",
                            "unk__8522",
                            3
                        ],
                        "type": "tensor(uint8)"
                    }
                ],
                "outputs": [
                    {
                        "name": "detection_boxes:0",
                        "shape": [
                            "unk__8523",
                            "unk__8524",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_classes:0",
                        "shape": [
                            "unk__8525",
                            "unk__8526"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_scores:0",
                        "shape": [
                            "unk__8527",
                            "unk__8528"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "num_detections:0",
                        "shape": [
                            "unk__8529"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_10.tar.gz",
            "model_with_data_sha": "b00add7ea1ffae33d879e6cf1498ab70dc828c07439528643ea5b9ba47474fd8",
            "model_with_data_bytes": 27902184
        }
    },
    {
        "model": "SSD",
        "model_path": "vision/object_detection_segmentation/ssd/model/ssd-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "80013dd940efc1462c616143b461ef35885bcc1706e0670b66623293b4b52c59",
            "model_bytes": 80363696,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            1,
                            3,
                            1200,
                            1200
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "bboxes",
                        "shape": [
                            1,
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "labels",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "scores",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd/model/ssd-10.tar.gz",
            "model_with_data_sha": "5ff54f161d58e02bfc751d3264d58d484485785e71827cf2dd14620bf354c2d0",
            "model_with_data_bytes": 78515627
        }
    },
    {
        "model": "Tiny YOLOv2",
        "model_path": "vision/object_detection_segmentation/tiny-yolov2/model/tinyyolov2-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "87befe217358b6beda0b496536b17216ebddef8f70e8d86fe34ed089bb577289",
            "model_bytes": 63480982,
            "tags": [
                "vision",
                "object detection segmentation",
                "tiny-yolov2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            "None",
                            3,
                            416,
                            416
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "grid",
                        "shape": [
                            "None",
                            125,
                            13,
                            13
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/tiny-yolov2/model/tinyyolov2-7.tar.gz",
            "model_with_data_sha": "4bcdc5e4884784c13bdc131f418040129885960e98134fc1bac7a97373418023",
            "model_with_data_bytes": 60875841
        }
    },
    {
        "model": "Tiny YOLOv2",
        "model_path": "vision/object_detection_segmentation/tiny-yolov2/model/tinyyolov2-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "583fb7fdc948435ceac9fa82efc7708701efe8382a859a3dd46526b155f5f2ae",
            "model_bytes": 63480982,
            "tags": [
                "vision",
                "object detection segmentation",
                "tiny-yolov2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            "None",
                            3,
                            416,
                            416
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "grid",
                        "shape": [
                            "None",
                            125,
                            13,
                            13
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/tiny-yolov2/model/tinyyolov2-8.tar.gz",
            "model_with_data_sha": "0d64e5b246b6ffa5ba9defcf72e726ac2dcfc97a8e1a79b43bf184f3caad7ea5",
            "model_with_data_bytes": 60875831
        }
    },
    {
        "model": "Tiny YOLOv3",
        "model_path": "vision/object_detection_segmentation/tiny-yolov3/model/tiny-yolov3-11.onnx",
        "onnx_version": "1.6",
        "opset_version": 11,
        "metadata": {
            "model_sha": "f715cc2d99740d22d312777e20d9de2b2ecdc250155be8fd3752ce7e8b823521",
            "model_bytes": 35511756,
            "tags": [
                "vision",
                "object detection segmentation",
                "tiny-yolov3"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_1",
                        "shape": [
                            "N",
                            3,
                            null,
                            null
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "image_shape",
                        "shape": [
                            "N",
                            2
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "yolonms_layer_1",
                        "shape": [
                            1,
                            null,
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1:1",
                        "shape": [
                            1,
                            80,
                            null
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1:2",
                        "shape": [
                            1,
                            null,
                            3
                        ],
                        "type": "tensor(int32)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/tiny-yolov3/model/tiny-yolov3-11.tar.gz",
            "model_with_data_sha": "acdf6c64b01ad574c419efe25d8babcc17d10d49e73cd1f636b1394e1af914ee",
            "model_with_data_bytes": 34058193
        }
    },
    {
        "model": "YOLOv2",
        "model_path": "vision/object_detection_segmentation/yolov2-coco/model/yolov2-coco-9.onnx",
        "onnx_version": "1.5",
        "opset_version": 9,
        "metadata": {
            "model_sha": "a2c44ecf4860acdf03193d41b7d2957637d0b14b8a9e339463b892b0acb9a12f",
            "model_bytes": 203948401,
            "tags": [
                "vision",
                "object detection segmentation",
                "yolov2-coco"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input.1",
                        "shape": [
                            1,
                            3,
                            416,
                            416
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "218",
                        "shape": [
                            1,
                            425,
                            13,
                            13
                        ],
                        "type": "tensor(float)"
                    }
                ]
            }
        }
    },
    {
        "model": "YOLOv3",
        "model_path": "vision/object_detection_segmentation/yolov3/model/yolov3-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "1f4613c3d04416dfd2c1960b8737aa5292994238dfecbe9c1ee7147e9a92439f",
            "model_bytes": 247908721,
            "tags": [
                "vision",
                "object detection segmentation",
                "yolov3"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_1",
                        "shape": [
                            "unk__576",
                            3,
                            "unk__577",
                            "unk__578"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "image_shape",
                        "shape": [
                            "unk__579",
                            2
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "yolonms_layer_1/ExpandDims_1:0",
                        "shape": [
                            1,
                            "unk__581",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1/ExpandDims_3:0",
                        "shape": [
                            1,
                            80,
                            "unk__584"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1/concat_2:0",
                        "shape": [
                            "unk__585",
                            "unk__586"
                        ],
                        "type": "tensor(int32)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/yolov3/model/yolov3-10.tar.gz",
            "model_with_data_sha": "13ca9e9fba6f2f1bcf31d62cf9b9586a04c49b5f2b8c23bc288b9b6706126c97",
            "model_with_data_bytes": 232571698
        }
    },
    {
        "model": "YOLOv4",
        "model_path": "vision/object_detection_segmentation/yolov4/model/yolov4.onnx",
        "onnx_version": "1.6",
        "opset_version": 11,
        "metadata": {
            "model_sha": "1881fe9c506c970d7866cb4bfc33bda791ce46951a3c39c45ace2ff2b9daf369",
            "model_bytes": 257470589,
            "tags": [
                "vision",
                "object detection segmentation",
                "yolov4"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_1:0",
                        "shape": [
                            "unk__2104",
                            416,
                            416,
                            3
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Identity:0",
                        "shape": [
                            "unk__2105",
                            "unk__2106",
                            "unk__2107",
                            3,
                            85
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "Identity_1:0",
                        "shape": [
                            "unk__2108",
                            "unk__2109",
                            "unk__2110",
                            3,
                            85
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "Identity_2:0",
                        "shape": [
                            "unk__2111",
                            "unk__2112",
                            "unk__2113",
                            3,
                            85
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/yolov4/model/yolov4.tar.gz",
            "model_with_data_sha": "b2fce1287242740a139bc8b15cf463fd4b50f15f480f1461d70e454f75ed0404",
            "model_with_data_bytes": 248665735
        }
    },
    {
        "model": "Mosaic",
        "model_path": "vision/style_transfer/fast_neural_style/model/mosaic-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "fa646dedade881243f8d5a2ceb7de2b93675b21fc24f7482894ac4851a9a0a47",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/mosaic-9.tar.gz",
            "model_with_data_sha": "e7d395a1a2d8776a2bdf27adf9b581a5bb2dc94d2ea47c54791cdc55afcae7a0",
            "model_with_data_bytes": 7333449
        }
    },
    {
        "model": "Candy",
        "model_path": "vision/style_transfer/fast_neural_style/model/candy-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "9d11a3529d1e547da6ae07201d93484dbab2ec0a3614535752c8f40f0fe2968a",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/candy-9.tar.gz",
            "model_with_data_sha": "7647c08408079b400f95526bf15e21a247af2112ca7019396fc6945b76f9cb33",
            "model_with_data_bytes": 7338825
        }
    },
    {
        "model": "Rain Princess",
        "model_path": "vision/style_transfer/fast_neural_style/model/rain-princess-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "4162912e6f75fedef6f810ae989b9e10d3d5d43308dab34b027c850cf255e152",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/rain-princess-9.tar.gz",
            "model_with_data_sha": "8c35f1d7c72c3d89ff2b61163a9715b3701c80603a38b6df28373162d48e895a",
            "model_with_data_bytes": 7344974
        }
    },
    {
        "model": "Udnie",
        "model_path": "vision/style_transfer/fast_neural_style/model/udnie-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "8656b6ce7dec8f22ee13c2d557d6b67bd6f550dde88d0f2e7c9972aeb765cc0d",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/udnie-9.tar.gz",
            "model_with_data_sha": "713f43d88764577218b25e4820b4fc75fe1b547d59c9bfcfa0349cbdbfd84a1e",
            "model_with_data_bytes": 7338543
        }
    },
    {
        "model": "Pointilism",
        "model_path": "vision/style_transfer/fast_neural_style/model/pointilism-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "5ee2b8d4d6bc60a777f54e0fe96a1b717360a004b79d56c67390d4a975b14d98",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/pointilism-9.tar.gz",
            "model_with_data_sha": "ceac242d7e058391b317fccc656c43be9e58a148036027dd38d6b1bd4ac040fd",
            "model_with_data_bytes": 7335554
        }
    },
    {
        "model": "Mosaic",
        "model_path": "vision/style_transfer/fast_neural_style/model/mosaic-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "8547ad01962f2709c2ca6732345c74d14cb1dd79c09c92365ac113cdb6c2eac5",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/mosaic-8.tar.gz",
            "model_with_data_sha": "2b3090c84a0e28fe00fc162b764c8946186a33535eee6250e2595429ac309346",
            "model_with_data_bytes": 7333067
        }
    },
    {
        "model": "Candy",
        "model_path": "vision/style_transfer/fast_neural_style/model/candy-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "e3c6f442e12014c5fec12b4307c14ca303df185cda19f67522afeef40c4d22b8",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/candy-8.tar.gz",
            "model_with_data_sha": "e6a40c8d1b5f6b680f4c770ac9ec5a26638fea1439bb2561e71a3a4a55c1e8a7",
            "model_with_data_bytes": 7338783
        }
    },
    {
        "model": "Rain Princess",
        "model_path": "vision/style_transfer/fast_neural_style/model/rain-princess-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "bf13b623576021a77dab6aa54c366d0900e8f55909be27cf4a40248d6ff6e097",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/rain-princess-8.tar.gz",
            "model_with_data_sha": "483c273fd3c293417250329cb9c64d68c19c1cf5a1c8e4a375ff92a00b3cb972",
            "model_with_data_bytes": 7344452
        }
    },
    {
        "model": "Udnie",
        "model_path": "vision/style_transfer/fast_neural_style/model/udnie-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "31f8c5457a0d850b55564b018f6e494e49b3392c50eee6662799a47491b4a933",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/udnie-8.tar.gz",
            "model_with_data_sha": "81768986c46484c6f48f6c43d8ba267a9fa52e5eed691239e60c26e3f672299c",
            "model_with_data_bytes": 7339429
        }
    },
    {
        "model": "Pointilism",
        "model_path": "vision/style_transfer/fast_neural_style/model/pointilism-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "09fb31931c1826daf42552e451a908d068486a3ca946324a3ece758d51f46cad",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/pointilism-8.tar.gz",
            "model_with_data_sha": "e67096dba8f00a30b86422868563f1c299565592314778e42033b87d46ba1ca8",
            "model_with_data_bytes": 7333381
        }
    },
    {
        "model": "Super_Resolution",
        "model_path": "vision/super_resolution/sub_pixel_cnn_2016/model/super-resolution-10.onnx",
        "onnx_version": "1.5.0",
        "opset_version": 10,
        "metadata": {
            "model_sha": "85f36ff88cc504a24af5e0602148bc56a8aa09a58eca8c0da2756f3e8186035e",
            "model_bytes": 240078,
            "tags": [
                "vision",
                "super resolution",
                "sub pixel cnn 2016"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch_size",
                            1,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            "batch_size",
                            1,
                            672,
                            672
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/super_resolution/sub_pixel_cnn_2016/model/super-resolution-10.tar.gz",
            "model_with_data_sha": "e42d465bfbed87ea770a7a856ba5d0c62b7fa8e2e0c260e6750e82ce9ddb72f2",
            "model_with_data_bytes": 1815286
        }
    }
]