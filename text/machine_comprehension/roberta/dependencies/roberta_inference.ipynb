{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from alibi.explainers import AnchorText\n",
    "from alibi.datasets import fetch_movie_sentiment\n",
    "from alibi.utils.download import spacy_model\n",
    "\n",
    "import torch\n",
    "\n",
    "from simpletransformers.model import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = fetch_movie_sentiment()\n",
    "movies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movies.data\n",
    "labels = movies.target\n",
    "target_names = movies.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel('roberta', 'roberta-base', args=({'fp16': False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.load_state_dict(torch.load('pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: model.predict(x)[1].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753c16ea0d1349268064d55418bee1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "explainer = AnchorText(nlp, predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = movies.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This film is so bad\n"
     ]
    }
   ],
   "source": [
    "text = \"This film is so bad\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c54abd393c464ab80dd4faea8618bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6623ede4b84de9bb0e12340d9abb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative =  class_names[1 - predict_fn([text])[0]]\n",
    "print('Prediction: %s' % pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2a942838674dbdbfae4ca7bbe7e771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7342eb27caa4429ba4bb7560919b0c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.12it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0371709d70462f9d611398b294ca98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:21<00:00,  1.63s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d028bea9cf4b46ada45e2ebe8711ee54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:21<00:00,  1.65s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca72c04655ab4af7a06bc1f11dee8249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:20<00:00,  1.60s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2e17910b2c42bcad1523e40f8d1200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "explanation = explainer.explain(text, threshold=0.95, use_unk=True, sample_proba=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e690922b544f78a36c19609a48d245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-09764e43503f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#np.random.seed(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexplanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_proba\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/alibi/explainers/anchor_text.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, text, threshold, delta, tau, batch_size, top_n, desired_label, use_similarity_proba, use_unk, sample_proba, temperature, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m                                          \u001b[0mdesired_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                                          \u001b[0mstop_on_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                                          **kwargs)  # type: Any\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/alibi/explainers/anchor_base.py\u001b[0m in \u001b[0;36manchor_beam\u001b[0;34m(sample_fn, delta, epsilon, batch_size, desired_confidence, beam_size, verbose, epsilon_stop, min_samples_start, max_anchor_size, verbose_every, stop_on_first, coverage_samples, data_type, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;31m# random (b/c first argument is empty) sample nb of coverage_samples from training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoverage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoverage_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;31m# sample by default 1 or min_samples_start more random value(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/alibi/explainers/anchor_text.py\u001b[0m in \u001b[0;36msample_fn\u001b[0;34m(present, num_samples, compute_labels)\u001b[0m\n\u001b[1;32m    176\u001b[0m                                                        \u001b[0muse_similarity_proba\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_similarity_proba\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                                                        \u001b[0msample_proba\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                                        **kwargs)\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# create labels using model predictions as true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/alibi/explainers/anchor_text.py\u001b[0m in \u001b[0;36mperturb_sentence\u001b[0;34m(self, text, present, n, sample_proba, top_n, forbidden, forbidden_tags, forbidden_words, pos, use_similarity_proba, temperature, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m                         \u001b[0mweights\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchanged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_changed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchanged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "\n",
    "#np.random.seed(0)\n",
    "explanation = explainer.explain(text, threshold=0.95, use_unk=False, sample_proba=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: ['bad']\n",
      "Precision: 1.00\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "UNK film is UNK bad\n",
      "This UNK is UNK bad\n",
      "This UNK is so bad\n",
      "This film UNK UNK bad\n",
      "This UNK UNK so bad\n",
      "UNK film is UNK bad\n",
      "UNK film is UNK bad\n",
      "UNK film UNK UNK bad\n",
      "This film is UNK bad\n",
      "This film is UNK bad\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Anchor: %s' % explanation['names'])\n",
    "print('Precision: %.2f' % explanation['precision'])\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x[0] for x in explanation['raw']['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x[0] for x in explanation['raw']['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy\n",
    "import time\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import logging\n",
    "import coloredlogs\n",
    "import torch\n",
    "import onnx\n",
    "from enum import Enum\n",
    "from packaging import version\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_latency_result(runtimes, batch_size):\n",
    "    latency_ms = sum(runtimes) / float(len(runtimes)) * 1000.0\n",
    "    latency_variance = numpy.var(runtimes, dtype=numpy.float64) * 1000.0\n",
    "    throughput = batch_size * (1000.0 / latency_ms)\n",
    "\n",
    "    return {\n",
    "        \"test_times\": len(runtimes),\n",
    "        \"latency_variance\": \"{:.2f}\".format(latency_variance),\n",
    "        \"latency_90_percentile\": \"{:.2f}\".format(numpy.percentile(runtimes, 90) * 1000.0),\n",
    "        \"latency_95_percentile\": \"{:.2f}\".format(numpy.percentile(runtimes, 95) * 1000.0),\n",
    "        \"latency_99_percentile\": \"{:.2f}\".format(numpy.percentile(runtimes, 99) * 1000.0),\n",
    "        \"average_latency_ms\": \"{:.2f}\".format(latency_ms),\n",
    "        \"QPS\": \"{:.2f}\".format(throughput),\n",
    "    }\n",
    "\n",
    "def inference_ort(ort_session, ort_inputs, result_template, repeat_times, batch_size):\n",
    "    result = {}\n",
    "    runtimes = timeit.repeat(lambda: ort_session.run(None, ort_inputs), number=1, repeat=repeat_times)\n",
    "    result.update(result_template)\n",
    "    result.update({\"io_binding\": False})\n",
    "    result.update(get_latency_result(runtimes, batch_size))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import torch \n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"benchmark_helper\")))\n",
    "# sys.path.append(BASE_DIR)\n",
    "\n",
    "#from benchmark_helper import inference_ort\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"roberta_fp16.onnx\")\n",
    "# if ort_session is None:\n",
    "#     continue\n",
    "\n",
    "#             ort_output_names = [node_arg.name for node_arg in ort_session.get_outputs()]\n",
    "output_buffers = {\"last_state\": None, \"pooler\": None}\n",
    "#             device = \"cpu\"\n",
    "#             config = AutoConfig.from_pretrained(\"roberta_fp16.onnx\", cache_dir=cache_dir)\n",
    "#             max_last_state_size = numpy.prod(\n",
    "#                 [max(batch_sizes), max(sequence_lengths),\n",
    "#                  max(vocab_size, config.hidden_size)])\n",
    "#             max_pooler_size = numpy.prod([max(batch_sizes), config.hidden_size])\n",
    "#             for batch_size in batch_sizes:\n",
    "#                 if batch_size <= 0:\n",
    "#                     continue\n",
    "#                 for sequence_length in sequence_lengths:\n",
    "#                     if max_sequence_length is not None and sequence_length > max_sequence_length:\n",
    "#                         continue\n",
    "\n",
    "results = []\n",
    "\n",
    "device = \"cpu\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "input_ids = torch.tensor(tokenizer.encode(\"This film is so bad\", add_special_tokens=True)).unsqueeze(0)\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(input_ids)}\n",
    "batch_size = input_ids.shape[0]\n",
    "sequence_length = input_ids.shape[1]\n",
    "\n",
    "result_template = {\n",
    "    \"engine\": \"onnxruntime\",\n",
    "     \"version\": onnxruntime.__version__,\n",
    "     \"device\": device,\n",
    "#      \"optimizer\": optimize_onnx,\n",
    "#       \"precision\": precision,\n",
    "#       \"io_binding\": False,\n",
    "    \"model_name\": \"roberta_fp16.onnx\",\n",
    "#       \"inputs\": num_inputs,\n",
    "     \"batch_size\": batch_size,\n",
    "      \"sequence_length\": sequence_length,\n",
    "       \"datetime\": str(datetime.now()),\n",
    "}\n",
    "logger.info(\"Run onnxruntime on {} with input shape {}\".format(\"roberta_fp16.onnx\",\n",
    "                                                                                   [batch_size, sequence_length]))\n",
    "result = inference_ort(ort_session, ort_inputs, result_template, 1, batch_size)\n",
    "logger.info(result)\n",
    "results.append(result)\n",
    "\n",
    "#                     if not disable_ort_io_binding:\n",
    "#                         logger.info(\"Run onnxruntime with io binding on {} with input shape {}\".format(\n",
    "#                             \"roberta_fp16.onnx\", [batch_size, sequence_length]))\n",
    "#                         # Get output sizes from a dummy ort run\n",
    "#                         ort_outputs = ort_session.run(ort_output_names, ort_inputs)\n",
    "#                         result = inference_ort_with_io_binding(ort_session, ort_inputs, result_template, repeat_times,\n",
    "#                                                                ort_output_names, ort_outputs, output_buffers)\n",
    "#                         logger.info(result)\n",
    "#                         results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'engine': 'onnxruntime', 'version': '1.3.0', 'device': 'cpu', 'model_name': 'roberta_fp16.onnx', 'batch_size': 1, 'sequence_length': 7, 'datetime': '2020-07-21 23:53:30.953123', 'io_binding': False, 'test_times': 1, 'latency_variance': '0.00', 'latency_90_percentile': '555.32', 'latency_95_percentile': '555.32', 'latency_99_percentile': '555.32', 'average_latency_ms': '555.32', 'QPS': '1.80'}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
